{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fasfmyQ6oNTu",
        "outputId": "ebaa5cfb-0347-4318-e23f-78f89892ee10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install prettytable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulQvlAVTrG0j",
        "outputId": "cf38d5b9-87b4-4d27-ecfd-1dc807ef832f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install kagglehub if not already installed\n",
        "!pip install kagglehub\n",
        "\n",
        "# Mount Google Drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "JlSulLltsyno",
        "outputId": "3fed3963-52d9-4213-fd8b-a37430f9283b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dir_path = \"/content/drive/My Drive/lip_reading_data/outputs\"\\n\\n# Set the dimensions of each frame\\nheight, width, channels = 80, 112, 3\\n\\n# Initialize arrays to store the video frames and their corresponding labels\\nvideos = []\\nlabels = []\\ncounter = 0\\n\\n# Loop through each text file and extract the video frames\\nfor root, dirs, files in os.walk(dir_path):\\n    for file in files:\\n        if file == \"data.txt\":\\n            # Extract the label from the directory name\\n            label = root.split(\"/\")[-1]\\n            label = label.split(\"_\")[0]\\n            counter += 1\\n            print(counter, end=\" \")\\n\\n            with open(os.path.join(root, file), \\'r\\') as f:\\n                data_str = f.read()\\n\\n            # Evaluate the contents of the text file as a Python expression\\n            data_list = eval(data_str)\\n\\n            # Convert the list to a numpy array\\n            data_array = np.array(data_list)\\n\\n            # Reshape the data\\n            num_frames = len(data_list)\\n            frames = data_array.reshape((num_frames, height, width, channels))\\n\\n            # Append the frames and label\\n            videos.append(frames)\\n            labels.append(label)\\n\\nprint(\"\\nLabels:\", labels)\\n\\n# Convert to NumPy arrays\\nvideos = np.array(videos)\\nlabels = np.array(labels)\\n\\n# Save the processed arrays\\nnp.save(\"/content/drive/My Drive/lip_reading_data/videosCorrect.npy\", videos)\\nnp.save(\"/content/drive/My Drive/lip_reading_data/labelsCorrect.npy\", labels)\\n\\nprint(\"\\nData saved successfully!\")\\nprint(\"Videos shape:\", videos.shape)\\nprint(\"Labels shape:\", labels.shape)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "'''dir_path = \"/content/drive/My Drive/lip_reading_data/outputs\"\n",
        "\n",
        "# Set the dimensions of each frame\n",
        "height, width, channels = 80, 112, 3\n",
        "\n",
        "# Initialize arrays to store the video frames and their corresponding labels\n",
        "videos = []\n",
        "labels = []\n",
        "counter = 0\n",
        "\n",
        "# Loop through each text file and extract the video frames\n",
        "for root, dirs, files in os.walk(dir_path):\n",
        "    for file in files:\n",
        "        if file == \"data.txt\":\n",
        "            # Extract the label from the directory name\n",
        "            label = root.split(\"/\")[-1]\n",
        "            label = label.split(\"_\")[0]\n",
        "            counter += 1\n",
        "            print(counter, end=\" \")\n",
        "\n",
        "            with open(os.path.join(root, file), 'r') as f:\n",
        "                data_str = f.read()\n",
        "\n",
        "            # Evaluate the contents of the text file as a Python expression\n",
        "            data_list = eval(data_str)\n",
        "\n",
        "            # Convert the list to a numpy array\n",
        "            data_array = np.array(data_list)\n",
        "\n",
        "            # Reshape the data\n",
        "            num_frames = len(data_list)\n",
        "            frames = data_array.reshape((num_frames, height, width, channels))\n",
        "\n",
        "            # Append the frames and label\n",
        "            videos.append(frames)\n",
        "            labels.append(label)\n",
        "\n",
        "print(\"\\nLabels:\", labels)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "videos = np.array(videos)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Save the processed arrays\n",
        "np.save(\"/content/drive/My Drive/lip_reading_data/videosCorrect.npy\", videos)\n",
        "np.save(\"/content/drive/My Drive/lip_reading_data/labelsCorrect.npy\", labels)\n",
        "\n",
        "print(\"\\nData saved successfully!\")\n",
        "print(\"Videos shape:\", videos.shape)\n",
        "print(\"Labels shape:\", labels.shape)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGgTBrPd-NDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b9db0f04-88ad-4ad1-b70e-c65813339d41"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/lip_reading_data/videosCorrect.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2cdbf53cd5ce>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/lip_reading_data/videosCorrect.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/lip_reading_data/labelsCorrect.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/lip_reading_data/videosCorrect.npy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "videos = np.load('/lip_reading_data/videosCorrect.npy')\n",
        "labels = np.load('/lip_reading_data/labelsCorrect.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from prettytable import PrettyTable\n",
        "import gc\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set Seaborn style directly\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "metadata": {
        "id": "jpU_Sc5sMwVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label dictionary\n",
        "label_dict = {\n",
        "    6: 'hello', 5: 'dog', 10: 'my', 12: 'you', 9: 'lips',\n",
        "    3: 'cat', 11: 'read', 0: 'a', 4: 'demo', 7: 'here',\n",
        "    8: 'is', 1: 'bye', 2: 'can'\n",
        "}\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)\n",
        "encoded_labels = encoder.transform(labels)\n",
        "labels = encoded_labels\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    videos, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Clear memory\n",
        "del videos\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "qwGNbkyJM3vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data_distribution():\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Bar plot\n",
        "    plt.subplot(121)\n",
        "    class_counts = pd.Series(labels).value_counts().sort_index()\n",
        "    sns.barplot(x=list(range(len(label_dict))),\n",
        "                y=class_counts.values,\n",
        "                palette='viridis')\n",
        "    plt.xticks(range(len(label_dict)),\n",
        "               [label_dict[i] for i in range(len(label_dict))],\n",
        "               rotation=45,\n",
        "               ha='right')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Pie chart\n",
        "    plt.subplot(122)\n",
        "    plt.pie(class_counts.values,\n",
        "            labels=[label_dict[i] for i in range(len(label_dict))],\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=90)\n",
        "    plt.title('Class Distribution (%)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print numerical distribution\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    for i in range(len(label_dict)):\n",
        "        print(f\"{label_dict[i]:<6}: {class_counts[i]} samples\")\n",
        "\n",
        "plot_data_distribution()"
      ],
      "metadata": {
        "id": "-mnDBLk7M7Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape=(22, 80, 112, 3), num_classes=13):\n",
        "    model = Sequential([\n",
        "        Conv3D(8, (3, 3, 3), activation='relu', input_shape=input_shape,\n",
        "               kernel_regularizer=regularizers.l2(0.001)),\n",
        "        MaxPooling3D((2, 2, 2)),\n",
        "        Conv3D(32, (3, 3, 3), activation='relu',\n",
        "               kernel_regularizer=regularizers.l2(0.001)),\n",
        "        MaxPooling3D((2, 2, 2)),\n",
        "        Conv3D(256, (3, 3, 3), activation='relu',\n",
        "               kernel_regularizer=regularizers.l2(0.001)),\n",
        "        Flatten(),\n",
        "        Dense(1024, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create and compile model\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "20LfROKVM_Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to one-hot encoding\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train_onehot,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_test, y_test_onehot),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training metrics\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JSDVvGd5NDzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights with correct extension\n",
        "model.save_weights('model_weights.weights.h5')  # Changed extension to .weights.h5\n",
        "\n",
        "# Get predictions\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Plot confusion matrix with enhanced styling\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[label_dict[i] for i in range(len(label_dict))],\n",
        "            yticklabels=[label_dict[i] for i in range(len(label_dict))])\n",
        "plt.title('Confusion Matrix', pad=20)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create detailed classification report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"]\n",
        "\n",
        "# Add rows to table with formatted metrics\n",
        "for label, metrics in report.items():\n",
        "    if label.isdigit():\n",
        "        class_name = label_dict[int(label)]\n",
        "        precision = f\"{metrics['precision']:.3f}\"\n",
        "        recall = f\"{metrics['recall']:.3f}\"\n",
        "        f1_score = f\"{metrics['f1-score']:.3f}\"\n",
        "        support = metrics['support']\n",
        "        table.add_row([class_name, precision, recall, f1_score, support])\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(table)\n",
        "\n",
        "# Add per-class accuracy visualization\n",
        "class_accuracy = {}\n",
        "for i in range(len(label_dict)):\n",
        "    mask = y_test == i\n",
        "    class_accuracy[label_dict[i]] = np.mean(y_pred[mask] == y_test[mask])\n",
        "\n",
        "# Plot per-class accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "classes = list(class_accuracy.keys())\n",
        "accuracies = list(class_accuracy.values())\n",
        "sns.barplot(x=classes, y=accuracies)\n",
        "plt.title('Per-Class Accuracy')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print overall model performance metrics\n",
        "print(\"\\nOverall Model Performance:\")\n",
        "print(f\"Average Accuracy: {np.mean(accuracies):.3f}\")\n",
        "print(f\"Best Performing Class: {classes[np.argmax(accuracies)]} ({max(accuracies):.3f})\")\n",
        "print(f\"Worst Performing Class: {classes[np.argmin(accuracies)]} ({min(accuracies):.3f})\")"
      ],
      "metadata": {
        "id": "87xo8VL2NHHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Positional Encoding\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    positions = np.arange(seq_len)[:, np.newaxis]\n",
        "    angles = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
        "    pos_encoding = positions * angles\n",
        "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# 2. Multi-Head Attention Layer\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
        "        scaled_attention_logits = scaled_attention_logits / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        return self.dense(output)\n",
        "\n",
        "# 3. Transformer Block\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu'),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        attn_output = self.mha(x, x, x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# 4. Feature Extraction Module\n",
        "class FeatureExtraction(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtraction, self).__init__()\n",
        "        self.conv1 = layers.Conv3D(32, (3, 3, 3), activation='relu')\n",
        "        self.pool1 = layers.MaxPool3D((1, 2, 2))\n",
        "        self.conv2 = layers.Conv3D(64, (3, 3, 3), activation='relu')\n",
        "        self.pool2 = layers.MaxPool3D((1, 2, 2))\n",
        "        self.conv3 = layers.Conv3D(128, (3, 3, 3), activation='relu')\n",
        "        self.pool3 = layers.MaxPool3D((1, 2, 2))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool3(x)\n",
        "        # Reshape for transformer input\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        seq_len = x.shape[1]\n",
        "        features = tf.reshape(x, (batch_size, seq_len, -1))\n",
        "        return features\n",
        "\n",
        "# 5. Complete Transformer Model\n",
        "class LipReadingTransformer(tf.keras.Model):\n",
        "    def __init__(self, num_classes, d_model=256, num_heads=8, dff=1024, num_blocks=6):\n",
        "        super(LipReadingTransformer, self).__init__()\n",
        "\n",
        "        self.feature_extraction = FeatureExtraction()\n",
        "        self.pos_encoding = get_positional_encoding(22, d_model)  # Adjust sequence length as needed\n",
        "\n",
        "        self.embedding = layers.Dense(d_model)\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(d_model, num_heads, dff)\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "\n",
        "        self.dropout = layers.Dropout(0.1)\n",
        "        self.final_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # Feature extraction\n",
        "        x = self.feature_extraction(x)\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        x = self.embedding(x)\n",
        "        x += self.pos_encoding[:tf.shape(x)[1], :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = tf.reduce_mean(x, axis=1)\n",
        "\n",
        "        # Final classification\n",
        "        return self.final_layer(x)\n",
        "\n",
        "# 6. Data Preprocessing Function\n",
        "def preprocess_data(videos, labels):\n",
        "    # Normalize pixel values\n",
        "    videos = videos.astype('float32') / 255.0\n",
        "\n",
        "    # Add temporal padding/truncation if needed\n",
        "    target_frames = 22  # Adjust based on your needs\n",
        "    current_frames = videos.shape[1]\n",
        "\n",
        "    if current_frames < target_frames:\n",
        "        pad_size = target_frames - current_frames\n",
        "        videos = np.pad(videos, ((0,0), (0,pad_size), (0,0), (0,0), (0,0)), mode='edge')\n",
        "    elif current_frames > target_frames:\n",
        "        videos = videos[:, :target_frames, :, :, :]\n",
        "\n",
        "    return videos, labels\n",
        "\n",
        "# 7. Training Configuration\n",
        "def get_training_config():\n",
        "    return {\n",
        "        'd_model': 256,\n",
        "        'num_heads': 8,\n",
        "        'dff': 1024,\n",
        "        'num_blocks': 4,\n",
        "        'batch_size': 16,\n",
        "        'epochs': 50,\n",
        "        'learning_rate': 0.0003,  # Adjusted for cosine decay\n",
        "        'gradient_clip_norm': 1.0  # Added gradient clipping\n",
        "    }\n",
        "\n",
        "# 8. Custom Learning Rate Schedule\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"d_model\": self.d_model.numpy(),\n",
        "            \"warmup_steps\": self.warmup_steps\n",
        "        }\n",
        "\n",
        "# 9. Training Loop\n",
        "def train_model(model, X_train, y_train, X_val, y_val, config):\n",
        "    # Use a constant learning rate with warmup and decay\n",
        "    initial_learning_rate = config['learning_rate']\n",
        "\n",
        "    class WarmupCosineDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "        def __init__(self, initial_learning_rate, warmup_steps, total_steps):\n",
        "            super().__init__()\n",
        "            self.initial_learning_rate = initial_learning_rate\n",
        "            self.warmup_steps = warmup_steps\n",
        "            self.total_steps = total_steps\n",
        "\n",
        "        def __call__(self, step):\n",
        "            # Warmup phase\n",
        "            warmup_progress = tf.cast(step, tf.float32) / tf.cast(self.warmup_steps, tf.float32)\n",
        "            warmup_lr = self.initial_learning_rate * warmup_progress\n",
        "\n",
        "            # Cosine decay phase\n",
        "            progress = tf.cast(step - self.warmup_steps, tf.float32) / tf.cast(self.total_steps - self.warmup_steps, tf.float32)\n",
        "            cosine_decay = 0.5 * (1.0 + tf.cos(tf.constant(np.pi) * progress))\n",
        "            decayed_lr = self.initial_learning_rate * cosine_decay\n",
        "\n",
        "            # Combine warmup and decay\n",
        "            return tf.where(step < self.warmup_steps, warmup_lr, decayed_lr)\n",
        "\n",
        "        def get_config(self):\n",
        "            return {\n",
        "                \"initial_learning_rate\": self.initial_learning_rate,\n",
        "                \"warmup_steps\": self.warmup_steps,\n",
        "                \"total_steps\": self.total_steps\n",
        "            }\n",
        "\n",
        "    # Calculate total steps\n",
        "    total_steps = (len(X_train) // config['batch_size']) * config['epochs']\n",
        "    warmup_steps = min(1000, total_steps // 10)  # 10% of total steps or 1000, whichever is smaller\n",
        "\n",
        "    # Create learning rate schedule\n",
        "    lr_schedule = WarmupCosineDecaySchedule(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        warmup_steps=warmup_steps,\n",
        "        total_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr_schedule,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.98,\n",
        "        epsilon=1e-9\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n",
        "            tf.keras.metrics.AUC(name='auc')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Add callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            monitor='val_accuracy'\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'transformer_lip_reading.keras',\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy'\n",
        "        ),\n",
        "        tf.keras.callbacks.TensorBoard(\n",
        "            log_dir='./logs',\n",
        "            histogram_freq=1,\n",
        "            update_freq='epoch'\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"Starting model training...\")\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Training labels shape: {y_train.shape}\")\n",
        "    print(f\"Validation data shape: {X_val.shape}\")\n",
        "    print(f\"Validation labels shape: {y_val.shape}\")\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=config['batch_size'],\n",
        "        epochs=config['epochs'],\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    videos = np.load('/lip_reading_data/videosCorrect.npy')\n",
        "    labels = np.load('/lip_reading_data/labelsCorrect.npy')\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Preprocess data\n",
        "    videos, encoded_labels = preprocess_data(videos, encoded_labels)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        videos, encoded_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Save label encoder mapping for later use\n",
        "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "    print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "    # Get training configuration\n",
        "    config = get_training_config()\n",
        "\n",
        "    # Initialize model\n",
        "    model = LipReadingTransformer(\n",
        "        num_classes=13,\n",
        "        d_model=config['d_model'],\n",
        "        num_heads=config['num_heads'],\n",
        "        dff=config['dff'],\n",
        "        num_blocks=config['num_blocks']\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    history = train_model(model, X_train, y_train, X_test, y_test, config)\n",
        "\n",
        "    # Save model\n",
        "    model.save_weights('transformer_lip_reading.weights.h5')"
      ],
      "metadata": {
        "id": "qwE6S4mNNJ4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, label_mapping):\n",
        "    # Get predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Convert label indices to actual words for better readability\n",
        "    label_mapping_inv = {v: k for k, v in label_mapping.items()}\n",
        "    y_pred_labels = [label_mapping_inv[i] for i in y_pred]\n",
        "    y_true_labels = [label_mapping_inv[i] for i in y_true]\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=list(label_mapping.keys()),\n",
        "                yticklabels=list(label_mapping.keys()))\n",
        "    plt.title('Confusion Matrix on Test Data')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "    # Calculate and plot per-class accuracy\n",
        "    class_accuracies = {}\n",
        "    for i in range(len(label_mapping)):\n",
        "        mask = y_true == i\n",
        "        class_accuracies[label_mapping_inv[i]] = np.mean(y_pred[mask] == y_true[mask])\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    classes = list(class_accuracies.keys())\n",
        "    accuracies = list(class_accuracies.values())\n",
        "    sns.barplot(x=classes, y=accuracies)\n",
        "    plt.title('Per-Class Accuracy on Test Data')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(\"\\nOverall Test Set Metrics:\")\n",
        "    print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
        "    print(f\"Best Performing Class: {classes[np.argmax(accuracies)]} ({max(accuracies):.4f})\")\n",
        "    print(f\"Worst Performing Class: {classes[np.argmin(accuracies)]} ({min(accuracies):.4f})\")\n",
        "\n",
        "    # Analyze misclassifications\n",
        "    misclassified_indices = np.where(y_pred != y_true)[0]\n",
        "    print(\"\\nMisclassified Examples Analysis:\")\n",
        "    if len(misclassified_indices) > 0:\n",
        "        print(f\"Total misclassified examples: {len(misclassified_indices)}\")\n",
        "        for idx in misclassified_indices:\n",
        "            true_label = label_mapping_inv[y_true[idx]]\n",
        "            pred_label = label_mapping_inv[y_pred[idx]]\n",
        "            confidence = y_pred_probs[idx][y_pred[idx]]\n",
        "            print(f\"True: {true_label}, Predicted: {pred_label}, Confidence: {confidence:.4f}\")\n",
        "    else:\n",
        "        print(\"No misclassifications found!\")\n",
        "\n",
        "    # Calculate confidence distribution\n",
        "    correct_confidences = []\n",
        "    incorrect_confidences = []\n",
        "    for i in range(len(y_pred)):\n",
        "        confidence = np.max(y_pred_probs[i])\n",
        "        if y_pred[i] == y_true[i]:\n",
        "            correct_confidences.append(confidence)\n",
        "        else:\n",
        "            incorrect_confidences.append(confidence)\n",
        "\n",
        "    # Plot confidence distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if correct_confidences:\n",
        "        plt.hist(correct_confidences, alpha=0.5, label='Correct Predictions',\n",
        "                bins=20, color='green')\n",
        "    if incorrect_confidences:\n",
        "        plt.hist(incorrect_confidences, alpha=0.5, label='Incorrect Predictions',\n",
        "                bins=20, color='red')\n",
        "    plt.title('Prediction Confidence Distribution')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "print(\"Starting model evaluation on test data...\")\n",
        "evaluate_model(model, X_test, y_test_onehot, label_mapping)"
      ],
      "metadata": {
        "id": "1bLqVP5AfrP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rStJZ08Nhkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class AttentionBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim//num_heads\n",
        "        )\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(\n",
        "            query=inputs,\n",
        "            key=inputs,\n",
        "            value=inputs\n",
        "        )\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class HybridModel(tf.keras.Model):\n",
        "    def __init__(self, num_classes=13, embed_dim=256, num_heads=8, ff_dim=512):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # CNN Feature Extractor\n",
        "        self.feature_extractor = tf.keras.Sequential([\n",
        "            layers.Conv3D(32, kernel_size=3, padding='same', activation='relu'),\n",
        "            layers.MaxPool3D(pool_size=(1, 2, 2)),\n",
        "            layers.Conv3D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "            layers.MaxPool3D(pool_size=(1, 2, 2)),\n",
        "            layers.Conv3D(128, kernel_size=3, padding='same', activation='relu'),\n",
        "            layers.MaxPool3D(pool_size=(1, 2, 2))\n",
        "        ])\n",
        "\n",
        "        # Projection layers\n",
        "        self.transformer_projection = layers.Dense(embed_dim)\n",
        "        self.gru_projection = layers.Dense(embed_dim)\n",
        "\n",
        "        # Transformer Path\n",
        "        self.attention_blocks = [\n",
        "            AttentionBlock(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(2)\n",
        "        ]\n",
        "\n",
        "        # GRU Path\n",
        "        self.gru1 = layers.Bidirectional(\n",
        "            layers.GRU(embed_dim//2, return_sequences=True,\n",
        "                      recurrent_dropout=0.1)\n",
        "        )\n",
        "        self.gru2 = layers.Bidirectional(\n",
        "            layers.GRU(embed_dim//2, return_sequences=True,\n",
        "                      recurrent_dropout=0.1)\n",
        "        )\n",
        "\n",
        "        # Fusion and Output layers\n",
        "        self.fusion_dense = layers.Dense(embed_dim)\n",
        "        self.dropout = layers.Dropout(0.5)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.final_dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # CNN Feature Extraction (B, T, H, W, C)\n",
        "        x = self.feature_extractor(inputs)\n",
        "\n",
        "        # Reshape for sequence processing (B, T, F)\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        features = tf.reshape(x, (batch_size, sequence_length, -1))\n",
        "\n",
        "        # Project features for both paths\n",
        "        trans_features = self.transformer_projection(features)\n",
        "        gru_features = self.gru_projection(features)\n",
        "\n",
        "        # Transformer Path\n",
        "        for attention_block in self.attention_blocks:\n",
        "            trans_features = attention_block(\n",
        "                inputs=trans_features,\n",
        "                training=training\n",
        "            )\n",
        "\n",
        "        # GRU Path\n",
        "        gru_output = self.gru1(gru_features, training=training)\n",
        "        gru_output = self.gru2(gru_output, training=training)\n",
        "\n",
        "        # Fusion with residual connection\n",
        "        fused = tf.concat([trans_features, gru_output], axis=-1)\n",
        "        fused = self.fusion_dense(fused)\n",
        "        fused = self.layer_norm(fused)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        pooled = tf.reduce_mean(fused, axis=1)\n",
        "\n",
        "        # Final Classification\n",
        "        x = self.dropout(pooled, training=training)\n",
        "        return self.final_dense(x)\n",
        "\n",
        "def train_model(X_train, y_train, X_val, y_val, epochs=50, batch_size=16):\n",
        "    print(f\"Training shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
        "    print(f\"Validation shapes - X: {X_val.shape}, y: {y_val.shape}\")\n",
        "\n",
        "    model = HybridModel()\n",
        "\n",
        "    # Use gradient clipping\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.001,\n",
        "        clipnorm=1.0\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'hybrid_model.keras',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Start training\n",
        "print(\"Starting model training...\")\n",
        "model, history = train_model(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "h2srpPTdgGTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, label_mapping):\n",
        "    # Get predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Convert indices to labels\n",
        "    label_mapping_inv = {v: k for k, v in label_mapping.items()}\n",
        "    y_pred_labels = [label_mapping_inv[i] for i in y_pred]\n",
        "    y_true_labels = [label_mapping_inv[i] for i in y_true]\n",
        "\n",
        "    # Calculate and plot confusion matrix\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=list(label_mapping.keys()),\n",
        "                yticklabels=list(label_mapping.keys()))\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    class_accuracies = {}\n",
        "    for i in range(len(label_mapping)):\n",
        "        mask = y_true == i\n",
        "        class_accuracies[label_mapping_inv[i]] = np.mean(y_pred[mask] == y_true[mask])\n",
        "\n",
        "    # Plot per-class accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    classes = list(class_accuracies.keys())\n",
        "    accuracies = list(class_accuracies.values())\n",
        "    sns.barplot(x=classes, y=accuracies)\n",
        "    plt.title('Per-Class Accuracy')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print top misclassifications\n",
        "    print(\"\\nAnalyzing Misclassifications:\")\n",
        "    misclassified = [(true, pred, prob) for true, pred, prob in\n",
        "                     zip(y_true_labels, y_pred_labels, np.max(y_pred_probs, axis=1))\n",
        "                     if true != pred]\n",
        "\n",
        "    if misclassified:\n",
        "        print(\"\\nTop Misclassifications:\")\n",
        "        for true, pred, prob in sorted(misclassified, key=lambda x: x[2], reverse=True)[:5]:\n",
        "            print(f\"True: {true}, Predicted: {pred}, Confidence: {prob:.4f}\")\n",
        "    else:\n",
        "        print(\"No misclassifications found!\")\n",
        "\n",
        "    # Plot confidence distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    correct_mask = y_pred == y_true\n",
        "    plt.hist(y_pred_probs[correct_mask].max(axis=1), alpha=0.5, label='Correct', bins=20)\n",
        "    if not correct_mask.all():  # Only plot incorrect if there are any\n",
        "        plt.hist(y_pred_probs[~correct_mask].max(axis=1), alpha=0.5, label='Incorrect', bins=20)\n",
        "    plt.title('Prediction Confidence Distribution')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
        "    print(f\"Best Performing Class: {classes[np.argmax(accuracies)]} ({max(accuracies):.4f})\")\n",
        "    print(f\"Worst Performing Class: {classes[np.argmin(accuracies)]} ({min(accuracies):.4f})\")\n",
        "\n",
        "# Example usage\n",
        "print(\"Starting model evaluation...\")\n",
        "evaluate_model(model, X_test, y_test, label_mapping)"
      ],
      "metadata": {
        "id": "z6Jm5pQtgCb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Recreate the model architecture\n",
        "def create_model(input_shape=(22, 80, 112, 3), num_classes=13):\n",
        "    model = Sequential([\n",
        "        Conv3D(8, (3, 3, 3), activation='relu', input_shape=input_shape,\n",
        "               kernel_regularizer=regularizers.l2(0.001)),\n",
        "        MaxPooling3D((2, 2, 2)),\n",
        "        Conv3D(32, (3, 3, 3), activation='relu',\n",
        "               kernel_regularizer=regularizers.l2(0.001)),\n",
        "        MaxPooling3D((2, 2, 2)),\n",
        "        Conv3D(256, (3, 3, 3), activation='relu',\n",
        "               kernel_regularizer=regularizers.l2(0.001)),\n",
        "        Flatten(),\n",
        "        Dense(1024, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Load the pre-trained weights\n",
        "model = create_model()\n",
        "model.load_weights('model_weights.weights.h5')\n",
        "\n",
        "# Define label dictionary (same as in training)\n",
        "label_dict = {\n",
        "    6: 'hello', 5: 'dog', 10: 'my', 12: 'you', 9: 'lips',\n",
        "    3: 'cat', 11: 'read', 0: 'a', 4: 'demo', 7: 'here',\n",
        "    8: 'is', 1: 'bye', 2: 'can'\n",
        "}\n",
        "\n",
        "def predict_single_video(video_path):\n",
        "    \"\"\"\n",
        "    Predict the label for a single video\n",
        "\n",
        "    Args:\n",
        "    video_path (str): Path to the video data file\n",
        "\n",
        "    Returns:\n",
        "    str: Predicted label\n",
        "    \"\"\"\n",
        "    # Read the data file\n",
        "    with open(video_path, 'r') as f:\n",
        "        data_str = f.read()\n",
        "\n",
        "    # Evaluate the contents of the text file as a Python expression\n",
        "    data_list = eval(data_str)\n",
        "\n",
        "    # Convert the list to a numpy array\n",
        "    data_array = np.array(data_list)\n",
        "\n",
        "    # Reshape the data (ensure it matches the training input shape)\n",
        "    height, width, channels = 80, 112, 3\n",
        "    num_frames = len(data_list)\n",
        "    frames = data_array.reshape((1, num_frames, height, width, channels))\n",
        "\n",
        "    # Make prediction\n",
        "    prediction_prob = model.predict(frames)\n",
        "    predicted_class_index = np.argmax(prediction_prob)\n",
        "\n",
        "    # Get the label\n",
        "    predicted_label = label_dict[predicted_class_index]\n",
        "\n",
        "    # Print detailed prediction information\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print(\"\\nPrediction Probabilities:\")\n",
        "    for idx, prob in enumerate(prediction_prob[0]):\n",
        "        print(f\"{label_dict[idx]}: {prob*100:.2f}%\")\n",
        "\n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "4jOdas7ugad_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_video_path = \"/lip_reading_data/outputs/cat_1/data.txt\"\n",
        "predicted_label = predict_single_video(sample_video_path)"
      ],
      "metadata": {
        "id": "CojeAikUmLbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0u50f3-mXP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}